{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Supervised Learning in a Nutshell</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/SupervisedLearning.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Classification</h1> \n",
    "<h2>\n",
    "<ul>\n",
    "<li>\n",
    "During training, a feature extraction scheme is used to convert each input value to a feature vectors. Labeled points, which consist of pairs of labels and feature vectors are fed into the machine learning algorithm to generate a model.\n",
    "</li>\n",
    "<p>\n",
    "<li>\n",
    "During prediction, the same feature extraction scheme is used to convert unseen inputs to feature vectors. These feature sets are then fed into the model, which generates predicted labels.\n",
    "</li>\n",
    "</ul>\n",
    "</h2>\n",
    "\n",
    "![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/Supervised-classification.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Organization of the dataset for training supervised classifiers</h1> \n",
    "<h2>\n",
    "<ul>\n",
    "<li>The dataset is divided into two sets: (1) the development set, and (2) the \"test\" set.</li>\n",
    "<p>\n",
    "<li>The development set can further be subdivided into a \"training\" set and a \"dev\"-test set.</li>\n",
    "</ul>\n",
    "</h2>\n",
    "\n",
    "![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/Corpus-org.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ML Pipeline</h1>\n",
    "<h2>In machine learning, it is common to run a sequence of algorithms to process and learn from data. \n",
    "<br>\n",
    "<p>A simple text document processing workflow might include several stages:\n",
    "<ol>\n",
    "<li>Split each document’s text into words.</li>\n",
    "<li>Convert each document’s words into a numerical feature vector.</li>\n",
    "<li>Learn a prediction model using the feature vectors and labels.</li>\n",
    "</ol>\n",
    "<br>\n",
    "Spark ML represents such a workflow as a <u>Pipeline</u>, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order.</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pipeline Components</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Transformers</h2>\n",
    "<h3>\n",
    "<ol>\n",
    "<li>A Transformer is an abstraction that includes feature transformers and learned models.\n",
    "</li>\n",
    "<p>\n",
    "<li>\n",
    "Technically, a Transformer implements a method transform(), which converts one DataFrame into another, generally by appending one or more columns.\n",
    "</li>\n",
    "<p>For example: \n",
    "<ul>\n",
    "<li>A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and <u>output a new DataFrame with the mapped column appended</u>.\n",
    "</li>\n",
    "<p>\n",
    "<li>A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and <u>output a new DataFrame with predicted labels appended as a column.</u>\n",
    "</li>\n",
    "</ul>\n",
    "</ol>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Estimators</h2>\n",
    "<h3>\n",
    "<ol>\n",
    "<li>An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. \n",
    "</li>\n",
    "<p>\n",
    "<li>\n",
    "Technically, <u>an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer</u>.\n",
    "<p>For example:\n",
    "<p>\n",
    "A learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\n",
    "</li>\n",
    "</ol>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/ml-Pipeline.png?raw=true)\n",
    "<hr size=\"5\">\n",
    "![](https://github.com/ahmetbulut/CS340withDSX/blob/master/static/Week9/ml-PipelineModel.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"SMSSpamCollection.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " u'ham\\tOk lar... Joking wif u oni...',\n",
       " u\"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " u'ham\\tU dun say so early hor... U c already then say...',\n",
       " u\"ham\\tNah I don't think he goes to usf, he lives around here though\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim = rdd.map(lambda line: (1.0 if line.split(\"\\t\")[0] == \"spam\" else 0.0, line.split(\"\\t\")[1]))\n",
    "interim = interim.map(lambda t: Row(label=t[0], text=t[1]))\n",
    "training = sqlContext.createDataFrame(interim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of 3 stages: \n",
    "# (1) Tokenizer, \n",
    "# (2) HashingTF, and \n",
    "# (3) LR.\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = sqlContext.createDataFrame([\n",
    "    Row(text=\"You will get a prize. To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"),\n",
    "    Row(text=\"Even my brother is not like to speak with me. They treat me like aids patent.\")])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|You will get a pr...|[you, will, get, ...|(262144,[14367,61...|[-0.3988733282427...|[0.40158306495705...|       1.0|\n",
      "|Even my brother i...|[even, my, brothe...|(262144,[2089,158...|[6.53076401421295...|[0.99854423077938...|       0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(text=u'You will get a prize. To claim call 09061701461. Claim code KL341. Valid 12 hours only.', prediction=1.0)\n",
      "Row(text=u'Even my brother is not like to speak with me. They treat me like aids patent.', prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "selected = prediction.select(\"text\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
