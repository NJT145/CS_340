{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "#sc = pyspark.SparkContext.getOrCreate()\n",
    "#sqlContext = pyspark.sql.SQLContext(sc)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType\n",
    "import xlrd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-IUICSM2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_excel_xlrd(path):\n",
    "    xl_workbook = xlrd.open_workbook(path)\n",
    "    #sheet_names = xl_workbook.sheet_names()\n",
    "    #print('Sheet Names', sheet_names)\n",
    "    #xl_sheet = xl_workbook.sheet_by_name(sheet_names[0])\n",
    "    xl_sheet = xl_workbook.sheet_by_index(0)\n",
    "    #print ('Sheet name: %s' % xl_sheet.name)\n",
    "    data = []\n",
    "    for row in range(xl_sheet.nrows):\n",
    "        line = []\n",
    "        for col in range(xl_sheet.ncols):\n",
    "            line.append(str(xl_sheet.cell(row, col).value))\n",
    "        data.append(line)\n",
    "    columnNames = data[0]\n",
    "    dataValues = data[1:]\n",
    "    dataPairs = []\n",
    "    for row in range(len(dataValues)):\n",
    "        line = {}\n",
    "        for col in range(len(columnNames)):\n",
    "            line[columnNames[col]] = dataValues[row][col]\n",
    "        dataPairs.append(Row(**line))\n",
    "    dataRDD = sc.parallelize(dataPairs)\n",
    "    df = dataRDD.toDF()\n",
    "    return df\n",
    "\n",
    "def topics_get(df, columnName, delimiter):\n",
    "    rows = df.groupBy(columnName).count().collect()\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        for item in row[columnName].split(delimiter):\n",
    "            if item not in results:\n",
    "                results.append(item)\n",
    "    results.sort()\n",
    "    return results\n",
    "\n",
    "def exist_list(list_all, list_compared):\n",
    "    results = []\n",
    "    for item in list_all:\n",
    "        if item in list_compared:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    return results\n",
    "\n",
    "def df_column_add(df, input_colName, output_colName, function, returnType):\n",
    "    function_udf = udf(lambda item: function(item), returnType)\n",
    "    return df.withColumn(output_colName, function_udf(col(input_colName)))\n",
    "\n",
    "def countEmptyAndNull(df):\n",
    "    for columnName in df.columns:\n",
    "        print(r\"Column '%s' has %d '', %d 'noInfo', %d 'None', %d filled, and %d null rows.\" \n",
    "              % (columnName, df.filter(df[columnName]=='').count(), df.filter(df[columnName]=='noInfo').count(),\n",
    "                 df.filter(df[columnName]=='None').count(), df.filter(df[columnName]!='').count(),\n",
    "                 df.filter(df[columnName].isNull()).count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "598\n",
      "598\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = read_excel_xlrd(\"lib-statistics.xlsx\")\n",
    "print(df.filter(df['Ödünç Sayısı'].isNull()).count())\n",
    "print(df.filter(df['Ödünç Sayısı']=='').count())\n",
    "df = df.withColumn('Sınıflama', split(df['Sınıflama'], \" \")[0])\n",
    "df = df.withColumn('Ödünç Sayısı', df['Ödünç Sayısı'].cast('float'))\n",
    "print(df.filter(df['Ödünç Sayısı'].isNull()).count())\n",
    "df = df.fillna({'Sınıflama' : 'noInfo', 'Eser Adı' : 'noInfo', 'Yazar' : 'noInfo', 'Dil' : 'noInfo', 'Konu Başlıkları' : 'noInfo' , 'Ödünç Sayısı' : 0.0})\n",
    "print(df.filter(df['Ödünç Sayısı'].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|sum(Ödünç Sayısı)|\n",
      "+-----------------+\n",
      "|            339.0|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum(df['Ödünç Sayısı'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dil: string (nullable = false)\n",
      " |-- Eser Adı: string (nullable = false)\n",
      " |-- Konu Başlıkları: string (nullable = false)\n",
      " |-- Sınıflama: string (nullable = false)\n",
      " |-- Yazar: string (nullable = false)\n",
      " |-- Ödünç Sayısı: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+--------------------+------------+\n",
      "|Dil|            Eser Adı|     Konu Başlıkları|Sınıflama|               Yazar|Ödünç Sayısı|\n",
      "+---+--------------------+--------------------+---------+--------------------+------------+\n",
      "|eng|Museum frictions ...|Museums--Social A...|       AM|                    |         0.0|\n",
      "|tur|Tahayyül gücünü y...|Imagination (Phil...|        B|                    |         0.0|\n",
      "|tur|Bir yol var : Min...|Ontology\n",
      "Stress M...|        B|        Damcı, Taner|         0.0|\n",
      "|tur|Psikoloji şerhi =...|Aristotle\n",
      "Aristot...|        B|            İbn Rüşd|         2.0|\n",
      "|eng|An examination of...|Özel, İsmet, 1944...|        B|      Kaya, Vefa Can|         0.0|\n",
      "|eng|An examination of...|Özel, İsmet, 1944...|        B|      Kaya, Vefa Can|         0.0|\n",
      "|tur|Ruhun uyanışı, ya...|Philosophy, Islam...|        B|İbn Ṭufeyl, Muḥam...|         0.0|\n",
      "|tur|Çağımızın sorunla...|Philosophy, Moder...|        B|   Russell, Bertrand|         1.0|\n",
      "|tur|Heidegger'in kulü...|Entity (Philosoph...|       BD|         Sharr, Adam|         1.0|\n",
      "|tur|İnsan ve tabiat =...|Philosophical Ant...|       BD|Nasr, Seyyid Hüseyin|         0.0|\n",
      "|tur|Erich Fromm'un ve...|Psychoanalysis\n",
      "Fr...|       BF|     Dobrenkov, V.İ.|         0.0|\n",
      "|tur|Hayatı kolaylaştı...|Happiness\n",
      "Persona...|       BF|        Yüter, Ahmet|         0.0|\n",
      "|tur|Üzüntüden kurtulm...|Worry\n",
      "Success\n",
      "Üzü...|       BF|ebu yusuf s-Sabba...|         0.0|\n",
      "|tur|Çağdaş yaşam ve n...|Psychology, Patho...|       BF|       Geçtan, Engin|         0.0|\n",
      "|tur|Düşünce gücüyle t...|Self-Actualizatio...|       BF|      Hay, Louise L.|         0.0|\n",
      "|tur|Bana bilgiçlik ta...|Sex Differences (...|       BF|     Solnit, Rebecca|         2.0|\n",
      "|tur|Sahip olmak ya da...|Personality\n",
      "Ontol...|       BF|        Fromm, Erich|         1.0|\n",
      "|tur|Geliştiren anne baba|Parent-Child Rela...|       BF|    Cüceloğlu, Doğan|         2.0|\n",
      "|eng|Research design a...|Psychology--Resea...|       BF| Bordens, Kenneth S.|         4.0|\n",
      "|tur|İslam büyüklerini...|Islamic Ethics\n",
      "İs...|       BJ|        İmam Şa’rani|         0.0|\n",
      "+---+--------------------+--------------------+---------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---------+--------------------+------------+\n",
      "|Dil|            Eser Adı|Konu Başlıkları|Sınıflama|               Yazar|Ödünç Sayısı|\n",
      "+---+--------------------+---------------+---------+--------------------+------------+\n",
      "|tur|Edebiyat tarihi y...|               |       PL|Erünsal, İsmail E.|         1.0|\n",
      "+---+--------------------+---------------+---------+--------------------+------------+\n",
      "\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "|Dil|Eser Adı|Konu Başlıkları|Sınıflama|Yazar|Ödünç Sayısı|\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "|Dil|Eser Adı|Konu Başlıkları|Sınıflama|Yazar|Ödünç Sayısı|\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "|Dil|Eser Adı|Konu Başlıkları|Sınıflama|Yazar|Ödünç Sayısı|\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "+---+--------+---------------+---------+-----+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Konu Başlıkları']=='').show()\n",
    "df.filter(df['Konu Başlıkları']=='noInfo').show()\n",
    "df.filter(df['Konu Başlıkları']=='None').show()\n",
    "df.filter(df['Konu Başlıkları'].isNull()).show()\n",
    "df.filter(df['Ödünç Sayısı'] != 0.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Dil' has 0 '', 0 'noInfo', 0 'None', 751 filled, and 0 null rows.\n",
      "Column 'Eser Adı' has 0 '', 0 'noInfo', 0 'None', 751 filled, and 0 null rows.\n",
      "Column 'Konu Başlıkları' has 1 '', 0 'noInfo', 0 'None', 750 filled, and 0 null rows.\n",
      "Column 'Sınıflama' has 0 '', 0 'noInfo', 0 'None', 751 filled, and 0 null rows.\n",
      "Column 'Yazar' has 294 '', 0 'noInfo', 0 'None', 457 filled, and 0 null rows.\n",
      "Column 'Ödünç Sayısı' has 0 '', 0 'noInfo', 0 'None', 0 filled, and 0 null rows.\n"
     ]
    }
   ],
   "source": [
    "countEmptyAndNull(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics = topics_get(df, 'Konu Başlıkları', '\\n')\n",
    "#print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_topics(item):\n",
    "    list1 = item.replace(' ', '_').split('\\n')\n",
    "    list1.sort()\n",
    "    return ' '.join(list1)\n",
    "\n",
    "def function_labels(item):\n",
    "    pass\n",
    "\n",
    "df_1 = df_column_add(df, 'Konu Başlıkları', 'topic titles', function_topics, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column Sınıflama must be of type NumericType but was actually of type StringType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    319\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o715.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column Sınıflama must be of type NumericType but was actually of type StringType.\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:73)\r\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:53)\r\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\r\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\r\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:278)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:265)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:278)\r\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-bec33a93bf31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashingTF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m test = sqlContext.createDataFrame([\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.6-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1160\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\APACHE~1\\Lib\\spark_lib\\spark-2.3.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column Sınıflama must be of type NumericType but was actually of type StringType.'"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='topic titles', outputCol='words')\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(df_1)\n",
    "\n",
    "test = sqlContext.createDataFrame([\n",
    "    Row(text=\"You will get a prize. To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"),\n",
    "    Row(text=\"Even my brother is not like to speak with me. They treat me like aids patent.\")])\n",
    "\n",
    "#prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|       751|\n",
      "+----------+\n",
      "\n",
      "+-----------------+\n",
      "|sum(Ödünç Sayısı)|\n",
      "+-----------------+\n",
      "|            339.0|\n",
      "+-----------------+\n",
      "\n",
      "0\n",
      "+---------+-----+\n",
      "|Sınıflama|count|\n",
      "+---------+-----+\n",
      "|        K|    3|\n",
      "|       UA|    1|\n",
      "|       LA|    4|\n",
      "|       BS|    4|\n",
      "|       NK|    2|\n",
      "|       AM|    1|\n",
      "|       PL|   90|\n",
      "|       PS|    7|\n",
      "|       DR|   83|\n",
      "|        F|    1|\n",
      "|       JF|    1|\n",
      "|       BP|  153|\n",
      "|       NX|    1|\n",
      "|        Q|    7|\n",
      "|       JC|    6|\n",
      "|       NA|    6|\n",
      "|       HV|    9|\n",
      "|       BX|    1|\n",
      "|       VA|    1|\n",
      "|        E|    1|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "66\n",
      "0\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "df2 = df.groupBy('Dil').count()\n",
    "df2.select(sum(df2['count'])).show()\n",
    "\n",
    "df.select(sum(df['Ödünç Sayısı'])).show()\n",
    "\n",
    "print(df.filter(df['Ödünç Sayısı'].isNull()).count())\n",
    "\n",
    "df.groupBy(df['Sınıflama']).count().show()\n",
    "\n",
    "df_s = df.groupBy(df['Sınıflama']).count()\n",
    "print(df_s.filter(df_s['count'] > 1).count())\n",
    "print(df_s.filter(df_s['count'] < 1).count())\n",
    "print(df_s.filter(df_s['count'] == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sınıflama|count|\n",
      "+---------+-----+\n",
      "|       BD|    2|\n",
      "|        D|   17|\n",
      "|       DD|    2|\n",
      "|       DF|    4|\n",
      "|       DG|    2|\n",
      "|       DK|    4|\n",
      "|       DR|   83|\n",
      "|       DS|   20|\n",
      "|       DT|    1|\n",
      "|       HD|    7|\n",
      "|       ND|    2|\n",
      "|       RD|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s.filter(df_s['Sınıflama'].like('%D%')).orderBy('Sınıflama').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+--------------------+\n",
      "|Sınıflama|collect_list(Konu Başlıkları)| collect_list(Yazar)|\n",
      "+---------+-----------------------------+--------------------+\n",
      "|        K|         [Family--Ethics--...|[Ataseven, Gülsen...|\n",
      "|       UA|         [North Atlantic T...| [Jordan, Robert S.]|\n",
      "|       LA|         [Cevdet, Mehmed, ...|[Ergin, Osman Nur...|\n",
      "|       BS|         [Turkish Literatu...|[Göçmenoğlu, Kası...|\n",
      "|       NK|         [Hilye-i Şerif--E...|[, Kuşoğlu, Mehme...|\n",
      "|       AM|         [Museums--Social ...|                  []|\n",
      "|       PL|         [Turkish Language...|[Yüzendağ, Ahmet,...|\n",
      "|       PS|         [Rich People--Fic...|[Fitzgerald, Fran...|\n",
      "|       DR|         [Insurgency--Serb...|[Özkan, Ayşe, , ,...|\n",
      "|        F|         [California, Sout...|       [Sanger, Kay]|\n",
      "|       JF|         [Executive Power-...|                  []|\n",
      "|       BP|         [Islam--Periodica...|[, , , , , , , , ...|\n",
      "|       NX|         [Piri Reis--Maps-...|                  []|\n",
      "|        Q|         [Science--Turkey-...|[Gökdoğan, Melek ...|\n",
      "|       JC|         [Civil Society\n",
      "Si...|[, Weiss, John, ,...|\n",
      "|       NA|         [Architecture--Sw...|[, Küçükdağ, Yusu...|\n",
      "|       HV|         [Foundations--Tur...|[Sak, İzzet, , , ...|\n",
      "|       BX|         [Abraham, Ecchell...|                  []|\n",
      "|       VA|         [France. Marine--...|   [Plouviez, David]|\n",
      "|        E|         [African American...|         [Malcolm X]|\n",
      "+---------+-----------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.groupBy(df['Sınıflama']).agg(collect_list('Konu Başlıkları').alias('gruplanmış konu başlıkları')).show()\n",
    "df.groupBy(df['Sınıflama']).agg(collect_list('Konu Başlıkları'), collect_list('Yazar')).show()\n",
    "#df.filter(df['Ödünç Sayısı'] != 0.0).select(\"Eser Adı\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = df.groupBy(split(split(df['Sınıflama'], \" \")[0], \"\")[0].alias(\"group\")).count().orderBy(\"group\")\n",
    "#group_count = df3.count()\n",
    "#groups = np.empty(group_count, dtype=\"S30\")\n",
    "#counts = np.empty(group_count)\n",
    "#for (index,row) in enumerate(df3.collect()):\n",
    "#    groups[index] = row['group']\n",
    "#    counts[index] = row['count']\n",
    "#sns.barplot(x=groups, y=counts)\n",
    "#plt.xticks(rotation='vertical')\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4 = df.groupBy(df['Dil']).count().orderBy('Dil')\n",
    "#lang_count = df4.count()\n",
    "#langs = np.empty(lang_count, dtype=\"S30\")\n",
    "#counts = np.empty(lang_count)\n",
    "#for (index,row) in enumerate(df4.collect()):\n",
    "#    langs[index] = row['Dil']\n",
    "#    counts[index] = row['count']\n",
    "#sns.barplot(x=langs, y=counts)\n",
    "#plt.xticks(rotation='vertical')\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|user|item|fav_items|\n",
      "+----+----+---------+\n",
      "|  u1|   1|[1, 2, 3]|\n",
      "|  u1|   4|[1, 2, 3]|\n",
      "+----+----+---------+\n",
      "\n",
      "+----+----+---------+------+\n",
      "|user|item|fav_items|result|\n",
      "+----+----+---------+------+\n",
      "|  u1|   1|[1, 2, 3]|     1|\n",
      "|  u1|   4|[1, 2, 3]|     0|\n",
      "+----+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "# First we create a RDD in order to create a dataFrame:\n",
    "rdd = sc.parallelize([('u1', 1, [1 ,2, 3]), ('u1', 4, [1, 2, 3])])\n",
    "df2 = rdd.toDF(['user', 'item', 'fav_items'])\n",
    "# Print dataFrame\n",
    "df2.show()\n",
    "\n",
    "# We make an user define function that receives two columns and do operation\n",
    "function = udf(lambda item, items: 1 if item in items else 0, IntegerType())\n",
    "\n",
    "df2.select('user', 'item', 'fav_items', function(col('item'), col('fav_items')).alias('result')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|items|\n",
      "+----+-----+\n",
      "|  u1|  a\n",
      "b|\n",
      "|  u1|d\n",
      "e\n",
      "a|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+------+\n",
      "|user|items|result|\n",
      "+----+-----+------+\n",
      "|  u1|  a\n",
      "b|   a b|\n",
      "|  u1|d\n",
      "e\n",
      "a| a d e|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType\n",
    "\n",
    "list_all1 = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "rdd = sc.parallelize([('u1', 'a\\nb'), ('u1', 'd\\ne\\na')])\n",
    "df1 = rdd.toDF(['user', 'items'])\n",
    "\n",
    "df1.show()\n",
    "\n",
    "def function2(item):\n",
    "    list1 = item.split('\\n')\n",
    "    list1.sort()\n",
    "    return ' '.join(list1)\n",
    "\n",
    "df_column_add(df1, 'items', 'result', function2, StringType()).show()\n",
    "\n",
    "def function3(item): return exist_list(list_all1, item.split('\\n'))\n",
    "\n",
    "#df_column_add(df1, 'items', 'result', function3, ArrayType(IntegerType())).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
